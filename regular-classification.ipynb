{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJ0S9kEasyjc"
   },
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "a60ZN1XJqjqs"
   },
   "outputs": [],
   "source": [
    "!pip install -q tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KclKS2uSqsTn"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Setting seeds for reproducibility.\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CAdE6uZs1bg"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nGNq1YE9quPL"
   },
   "outputs": [],
   "source": [
    "# DATA\n",
    "BUFFER_SIZE = 1024\n",
    "BATCH_SIZE = 256\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = (32, 32, 3)\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 5e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 100\n",
    "\n",
    "# AUGMENTATION\n",
    "IMAGE_SIZE = 48  # We'll resize input images to this size.\n",
    "PATCH_SIZE = 6  # Size of the patches to be extract from the input images.\n",
    "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
    "\n",
    "# ENCODER and DECODER\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "ENC_PROJECTION_DIM = 128\n",
    "ENC_NUM_HEADS = 4\n",
    "ENC_LAYERS = 3\n",
    "ENC_TRANSFORMER_UNITS = [\n",
    "    ENC_PROJECTION_DIM * 2,\n",
    "    ENC_PROJECTION_DIM,\n",
    "]  # Size of the transformer layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82XzhKTus3Ol"
   },
   "source": [
    "## CIFAR-10 dataset loading and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fNENQQyJqwDY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 40000\n",
      "Validation samples: 10000\n",
      "Testing samples: 10000\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "(x_train, y_train), (x_val, y_val) = (\n",
    "    (x_train[:40000], y_train[:40000]),\n",
    "    (x_train[40000:], y_train[40000:]),\n",
    ")\n",
    "print(f\"Training samples: {len(x_train)}\")\n",
    "print(f\"Validation samples: {len(x_val)}\")\n",
    "print(f\"Testing samples: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UkKfe5hWq19S"
   },
   "outputs": [],
   "source": [
    "def get_train_augmentation_model():\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            layers.Rescaling(1 / 255.0),\n",
    "            layers.Resizing(INPUT_SHAPE[0] + 20, INPUT_SHAPE[0] + 20),\n",
    "            layers.RandomCrop(IMAGE_SIZE, IMAGE_SIZE),\n",
    "            layers.RandomFlip(\"horizontal\"),\n",
    "        ],\n",
    "        name=\"train_data_augmentation\",\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_test_augmentation_model():\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            layers.Rescaling(1 / 255.0),\n",
    "            layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        ],\n",
    "        name=\"test_data_augmentation\",\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LlCeqaYaqxWu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-20 05:04:53.612591: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-20 05:04:54.098967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38444 MB memory:  -> device: 0, name: A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(images, labels, is_train=True):\n",
    "    if is_train:\n",
    "        augmentation_model = get_train_augmentation_model()\n",
    "    else:\n",
    "        augmentation_model = get_test_augmentation_model()\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "\n",
    "    dataset = dataset.batch(BATCH_SIZE).map(\n",
    "        lambda x, y: (augmentation_model(x), y), num_parallel_calls=AUTO\n",
    "    )\n",
    "    return dataset.prefetch(AUTO)\n",
    "\n",
    "\n",
    "train_ds = prepare_data(x_train, y_train)\n",
    "val_ds = prepare_data(x_train, y_train, is_train=False)\n",
    "test_ds = prepare_data(x_test, y_test, is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXWdbsOjs6ek"
   },
   "source": [
    "## Patchify layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "H5rZ-br7q3lL"
   },
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size=PATCH_SIZE):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hJ40Eh_9q9F-"
   },
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches=NUM_PATCHES, projection_dim=ENC_PROJECTION_DIM):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSgz58Uys_wN"
   },
   "source": [
    "## ViT model utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, dropout_rate, hidden_units):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "WDtU--GgrPGG"
   },
   "outputs": [],
   "source": [
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    # Create patches.\n",
    "    patches = Patches()(inputs)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder()(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(ENC_LAYERS):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=ENC_NUM_HEADS, key_dim=ENC_PROJECTION_DIM, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=ENC_TRANSFORMER_UNITS, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(encoded_patches)\n",
    "    representation = layers.GlobalAveragePooling1D()(representation)\n",
    "    # Classify outputs.\n",
    "    outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\")(representation)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihd604hFtCcg"
   },
   "source": [
    "## LR scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "v2wkvqhlsQ-M"
   },
   "outputs": [],
   "source": [
    "# Some code is taken from:\n",
    "# https://www.kaggle.com/ashusma/training-rfcx-tensorflow-tpu-effnet-b2.\n",
    "\n",
    "\n",
    "class WarmUpCosine(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(\n",
    "        self, learning_rate_base, total_steps, warmup_learning_rate, warmup_steps\n",
    "    ):\n",
    "        super(WarmUpCosine, self).__init__()\n",
    "\n",
    "        self.learning_rate_base = learning_rate_base\n",
    "        self.total_steps = total_steps\n",
    "        self.warmup_learning_rate = warmup_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.pi = tf.constant(np.pi)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        if self.total_steps < self.warmup_steps:\n",
    "            raise ValueError(\"Total_steps must be larger or equal to warmup_steps.\")\n",
    "\n",
    "        cos_annealed_lr = tf.cos(\n",
    "            self.pi\n",
    "            * (tf.cast(step, tf.float32) - self.warmup_steps)\n",
    "            / float(self.total_steps - self.warmup_steps)\n",
    "        )\n",
    "        learning_rate = 0.5 * self.learning_rate_base * (1 + cos_annealed_lr)\n",
    "\n",
    "        if self.warmup_steps > 0:\n",
    "            if self.learning_rate_base < self.warmup_learning_rate:\n",
    "                raise ValueError(\n",
    "                    \"Learning_rate_base must be larger or equal to \"\n",
    "                    \"warmup_learning_rate.\"\n",
    "                )\n",
    "            slope = (\n",
    "                self.learning_rate_base - self.warmup_learning_rate\n",
    "            ) / self.warmup_steps\n",
    "            warmup_rate = slope * tf.cast(step, tf.float32) + self.warmup_learning_rate\n",
    "            learning_rate = tf.where(\n",
    "                step < self.warmup_steps, warmup_rate, learning_rate\n",
    "            )\n",
    "        return tf.where(\n",
    "            step > self.total_steps, 0.0, learning_rate, name=\"learning_rate\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5PtvlXWYsXbD"
   },
   "outputs": [],
   "source": [
    "total_steps = int((len(x_train) / BATCH_SIZE) * EPOCHS)\n",
    "warmup_steps = int(total_steps * 0.15)\n",
    "scheduled_lrs = WarmUpCosine(\n",
    "    learning_rate_base=LEARNING_RATE,\n",
    "    total_steps=total_steps,\n",
    "    warmup_learning_rate=0.0,\n",
    "    warmup_steps=warmup_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnQzF-v3tE0X"
   },
   "source": [
    "## Train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "HT-gf8mpsZ5F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  7/157 [>.............................] - ETA: 4s - loss: 3.1052 - accuracy: 0.1004"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-20 05:05:00.709362: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 10s 38ms/step - loss: 2.0991 - accuracy: 0.2296 - val_loss: 1.8311 - val_accuracy: 0.3259\n",
      "Epoch 2/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 1.6671 - accuracy: 0.3765 - val_loss: 1.5496 - val_accuracy: 0.4329\n",
      "Epoch 3/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 1.4412 - accuracy: 0.4726 - val_loss: 1.4947 - val_accuracy: 0.4563\n",
      "Epoch 4/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 1.3576 - accuracy: 0.5055 - val_loss: 1.3952 - val_accuracy: 0.5098\n",
      "Epoch 5/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 1.2870 - accuracy: 0.5368 - val_loss: 1.2170 - val_accuracy: 0.5587\n",
      "Epoch 6/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 1.2470 - accuracy: 0.5504 - val_loss: 1.3161 - val_accuracy: 0.5267\n",
      "Epoch 7/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 1.2244 - accuracy: 0.5608 - val_loss: 1.2774 - val_accuracy: 0.5430\n",
      "Epoch 8/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 1.2032 - accuracy: 0.5678 - val_loss: 1.1813 - val_accuracy: 0.5835\n",
      "Epoch 9/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 1.1758 - accuracy: 0.5803 - val_loss: 1.1546 - val_accuracy: 0.5860\n",
      "Epoch 10/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 1.1518 - accuracy: 0.5871 - val_loss: 1.1106 - val_accuracy: 0.6019\n",
      "Epoch 11/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 1.1271 - accuracy: 0.5975 - val_loss: 1.0758 - val_accuracy: 0.6131\n",
      "Epoch 12/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 1.1118 - accuracy: 0.6036 - val_loss: 1.0557 - val_accuracy: 0.6286\n",
      "Epoch 13/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 1.0820 - accuracy: 0.6119 - val_loss: 1.0240 - val_accuracy: 0.6359\n",
      "Epoch 14/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 1.0534 - accuracy: 0.6248 - val_loss: 1.0000 - val_accuracy: 0.6433\n",
      "Epoch 15/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 1.0277 - accuracy: 0.6338 - val_loss: 0.9965 - val_accuracy: 0.6510\n",
      "Epoch 16/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 1.0188 - accuracy: 0.6394 - val_loss: 1.0012 - val_accuracy: 0.6446\n",
      "Epoch 17/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.9751 - accuracy: 0.6529 - val_loss: 0.9140 - val_accuracy: 0.6781\n",
      "Epoch 18/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.9499 - accuracy: 0.6623 - val_loss: 0.8709 - val_accuracy: 0.6935\n",
      "Epoch 19/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.9236 - accuracy: 0.6731 - val_loss: 0.9088 - val_accuracy: 0.6749\n",
      "Epoch 20/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.8991 - accuracy: 0.6798 - val_loss: 0.8495 - val_accuracy: 0.6970\n",
      "Epoch 21/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.8730 - accuracy: 0.6894 - val_loss: 0.8039 - val_accuracy: 0.7132\n",
      "Epoch 22/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.8738 - accuracy: 0.6899 - val_loss: 0.8118 - val_accuracy: 0.7095\n",
      "Epoch 23/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.8514 - accuracy: 0.6985 - val_loss: 0.7825 - val_accuracy: 0.7232\n",
      "Epoch 24/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.8264 - accuracy: 0.7094 - val_loss: 0.7806 - val_accuracy: 0.7223\n",
      "Epoch 25/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.8255 - accuracy: 0.7078 - val_loss: 0.8044 - val_accuracy: 0.7170\n",
      "Epoch 26/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.8063 - accuracy: 0.7153 - val_loss: 0.8318 - val_accuracy: 0.7056\n",
      "Epoch 27/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.7954 - accuracy: 0.7179 - val_loss: 0.7690 - val_accuracy: 0.7269\n",
      "Epoch 28/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.7938 - accuracy: 0.7175 - val_loss: 0.7561 - val_accuracy: 0.7327\n",
      "Epoch 29/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.7802 - accuracy: 0.7233 - val_loss: 0.7353 - val_accuracy: 0.7429\n",
      "Epoch 30/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.7652 - accuracy: 0.7278 - val_loss: 0.7132 - val_accuracy: 0.7476\n",
      "Epoch 31/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.7597 - accuracy: 0.7307 - val_loss: 0.7345 - val_accuracy: 0.7364\n",
      "Epoch 32/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.7503 - accuracy: 0.7325 - val_loss: 0.6879 - val_accuracy: 0.7538\n",
      "Epoch 33/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.7422 - accuracy: 0.7338 - val_loss: 0.7275 - val_accuracy: 0.7434\n",
      "Epoch 34/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.7242 - accuracy: 0.7416 - val_loss: 0.6863 - val_accuracy: 0.7588\n",
      "Epoch 35/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.7077 - accuracy: 0.7485 - val_loss: 0.7564 - val_accuracy: 0.7327\n",
      "Epoch 36/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.7072 - accuracy: 0.7491 - val_loss: 0.6337 - val_accuracy: 0.7742\n",
      "Epoch 37/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.6915 - accuracy: 0.7533 - val_loss: 0.6605 - val_accuracy: 0.7653\n",
      "Epoch 38/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.6896 - accuracy: 0.7548 - val_loss: 0.7734 - val_accuracy: 0.7308\n",
      "Epoch 39/100\n",
      "157/157 [==============================] - 6s 36ms/step - loss: 0.6797 - accuracy: 0.7567 - val_loss: 0.6568 - val_accuracy: 0.7689\n",
      "Epoch 40/100\n",
      "157/157 [==============================] - 6s 36ms/step - loss: 0.6676 - accuracy: 0.7632 - val_loss: 0.6202 - val_accuracy: 0.7815\n",
      "Epoch 41/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.6623 - accuracy: 0.7634 - val_loss: 0.6149 - val_accuracy: 0.7825\n",
      "Epoch 42/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.6568 - accuracy: 0.7669 - val_loss: 0.6047 - val_accuracy: 0.7865\n",
      "Epoch 43/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.6285 - accuracy: 0.7764 - val_loss: 0.5951 - val_accuracy: 0.7904\n",
      "Epoch 44/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.6373 - accuracy: 0.7718 - val_loss: 0.5747 - val_accuracy: 0.7963\n",
      "Epoch 45/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.6207 - accuracy: 0.7786 - val_loss: 0.5694 - val_accuracy: 0.7967\n",
      "Epoch 46/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.6150 - accuracy: 0.7802 - val_loss: 0.5547 - val_accuracy: 0.8016\n",
      "Epoch 47/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.5999 - accuracy: 0.7850 - val_loss: 0.5629 - val_accuracy: 0.8000\n",
      "Epoch 48/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.5938 - accuracy: 0.7887 - val_loss: 0.5304 - val_accuracy: 0.8109\n",
      "Epoch 49/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.5825 - accuracy: 0.7936 - val_loss: 0.5464 - val_accuracy: 0.8070\n",
      "Epoch 50/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.5735 - accuracy: 0.7944 - val_loss: 0.5285 - val_accuracy: 0.8102\n",
      "Epoch 51/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.5652 - accuracy: 0.7973 - val_loss: 0.4906 - val_accuracy: 0.8238\n",
      "Epoch 52/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.5542 - accuracy: 0.8020 - val_loss: 0.5000 - val_accuracy: 0.8212\n",
      "Epoch 53/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.5407 - accuracy: 0.8064 - val_loss: 0.5067 - val_accuracy: 0.8206\n",
      "Epoch 54/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.5318 - accuracy: 0.8085 - val_loss: 0.5140 - val_accuracy: 0.8174\n",
      "Epoch 55/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.5235 - accuracy: 0.8118 - val_loss: 0.5191 - val_accuracy: 0.8181\n",
      "Epoch 56/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.5132 - accuracy: 0.8161 - val_loss: 0.4507 - val_accuracy: 0.8372\n",
      "Epoch 57/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.5019 - accuracy: 0.8206 - val_loss: 0.4533 - val_accuracy: 0.8389\n",
      "Epoch 58/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.4870 - accuracy: 0.8250 - val_loss: 0.4676 - val_accuracy: 0.8334\n",
      "Epoch 59/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.4792 - accuracy: 0.8271 - val_loss: 0.4430 - val_accuracy: 0.8423\n",
      "Epoch 60/100\n",
      "157/157 [==============================] - 5s 35ms/step - loss: 0.4632 - accuracy: 0.8337 - val_loss: 0.4292 - val_accuracy: 0.8454\n",
      "Epoch 61/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.4584 - accuracy: 0.8332 - val_loss: 0.4205 - val_accuracy: 0.8507\n",
      "Epoch 62/100\n",
      "157/157 [==============================] - 5s 35ms/step - loss: 0.4396 - accuracy: 0.8426 - val_loss: 0.3823 - val_accuracy: 0.8639\n",
      "Epoch 63/100\n",
      "157/157 [==============================] - 5s 35ms/step - loss: 0.4307 - accuracy: 0.8425 - val_loss: 0.4040 - val_accuracy: 0.8534\n",
      "Epoch 64/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.4277 - accuracy: 0.8449 - val_loss: 0.3783 - val_accuracy: 0.8638\n",
      "Epoch 65/100\n",
      "157/157 [==============================] - 5s 35ms/step - loss: 0.4092 - accuracy: 0.8521 - val_loss: 0.3555 - val_accuracy: 0.8723\n",
      "Epoch 66/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.3866 - accuracy: 0.8610 - val_loss: 0.3624 - val_accuracy: 0.8692\n",
      "Epoch 67/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.3760 - accuracy: 0.8645 - val_loss: 0.3408 - val_accuracy: 0.8788\n",
      "Epoch 68/100\n",
      "157/157 [==============================] - 5s 35ms/step - loss: 0.3649 - accuracy: 0.8679 - val_loss: 0.3157 - val_accuracy: 0.8860\n",
      "Epoch 69/100\n",
      "157/157 [==============================] - 5s 35ms/step - loss: 0.3471 - accuracy: 0.8767 - val_loss: 0.3259 - val_accuracy: 0.8817\n",
      "Epoch 70/100\n",
      "157/157 [==============================] - 5s 35ms/step - loss: 0.3410 - accuracy: 0.8762 - val_loss: 0.3038 - val_accuracy: 0.8923\n",
      "Epoch 71/100\n",
      "157/157 [==============================] - 5s 35ms/step - loss: 0.3252 - accuracy: 0.8819 - val_loss: 0.2978 - val_accuracy: 0.8919\n",
      "Epoch 72/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.3119 - accuracy: 0.8886 - val_loss: 0.2736 - val_accuracy: 0.8999\n",
      "Epoch 73/100\n",
      "157/157 [==============================] - 5s 35ms/step - loss: 0.2995 - accuracy: 0.8931 - val_loss: 0.2683 - val_accuracy: 0.9031\n",
      "Epoch 74/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.2927 - accuracy: 0.8952 - val_loss: 0.2588 - val_accuracy: 0.9061\n",
      "Epoch 75/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.2798 - accuracy: 0.9001 - val_loss: 0.2364 - val_accuracy: 0.9141\n",
      "Epoch 76/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.2738 - accuracy: 0.9020 - val_loss: 0.2424 - val_accuracy: 0.9124\n",
      "Epoch 77/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.2561 - accuracy: 0.9084 - val_loss: 0.2404 - val_accuracy: 0.9125\n",
      "Epoch 78/100\n",
      "157/157 [==============================] - 5s 35ms/step - loss: 0.2538 - accuracy: 0.9087 - val_loss: 0.2352 - val_accuracy: 0.9134\n",
      "Epoch 79/100\n",
      "157/157 [==============================] - 5s 35ms/step - loss: 0.2415 - accuracy: 0.9136 - val_loss: 0.1984 - val_accuracy: 0.9286\n",
      "Epoch 80/100\n",
      "157/157 [==============================] - 5s 35ms/step - loss: 0.2296 - accuracy: 0.9196 - val_loss: 0.1937 - val_accuracy: 0.9296\n",
      "Epoch 81/100\n",
      "157/157 [==============================] - 5s 35ms/step - loss: 0.2221 - accuracy: 0.9226 - val_loss: 0.1878 - val_accuracy: 0.9316\n",
      "Epoch 82/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.2154 - accuracy: 0.9240 - val_loss: 0.1882 - val_accuracy: 0.9327\n",
      "Epoch 83/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.2109 - accuracy: 0.9266 - val_loss: 0.1754 - val_accuracy: 0.9364\n",
      "Epoch 84/100\n",
      "157/157 [==============================] - 5s 35ms/step - loss: 0.2041 - accuracy: 0.9292 - val_loss: 0.1679 - val_accuracy: 0.9401\n",
      "Epoch 85/100\n",
      "157/157 [==============================] - 5s 35ms/step - loss: 0.1931 - accuracy: 0.9336 - val_loss: 0.1544 - val_accuracy: 0.9464\n",
      "Epoch 86/100\n",
      "157/157 [==============================] - 5s 35ms/step - loss: 0.1896 - accuracy: 0.9349 - val_loss: 0.1435 - val_accuracy: 0.9488\n",
      "Epoch 87/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.1877 - accuracy: 0.9361 - val_loss: 0.1437 - val_accuracy: 0.9499\n",
      "Epoch 88/100\n",
      "157/157 [==============================] - 5s 35ms/step - loss: 0.1783 - accuracy: 0.9403 - val_loss: 0.1361 - val_accuracy: 0.9527\n",
      "Epoch 89/100\n",
      "157/157 [==============================] - 5s 35ms/step - loss: 0.1771 - accuracy: 0.9415 - val_loss: 0.1335 - val_accuracy: 0.9541\n",
      "Epoch 90/100\n",
      "157/157 [==============================] - 5s 35ms/step - loss: 0.1785 - accuracy: 0.9409 - val_loss: 0.1279 - val_accuracy: 0.9568\n",
      "Epoch 91/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.1793 - accuracy: 0.9413 - val_loss: 0.1261 - val_accuracy: 0.9583\n",
      "Epoch 92/100\n",
      "157/157 [==============================] - 5s 35ms/step - loss: 0.1747 - accuracy: 0.9438 - val_loss: 0.1231 - val_accuracy: 0.9603\n",
      "Epoch 93/100\n",
      "157/157 [==============================] - 5s 35ms/step - loss: 0.1794 - accuracy: 0.9423 - val_loss: 0.1276 - val_accuracy: 0.9588\n",
      "Epoch 94/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.1862 - accuracy: 0.9409 - val_loss: 0.1281 - val_accuracy: 0.9596\n",
      "Epoch 95/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.1847 - accuracy: 0.9431 - val_loss: 0.1343 - val_accuracy: 0.9576\n",
      "Epoch 96/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.1955 - accuracy: 0.9396 - val_loss: 0.1424 - val_accuracy: 0.9562\n",
      "Epoch 97/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.2114 - accuracy: 0.9334 - val_loss: 0.1544 - val_accuracy: 0.9532\n",
      "Epoch 98/100\n",
      "157/157 [==============================] - 5s 35ms/step - loss: 0.2262 - accuracy: 0.9309 - val_loss: 0.1716 - val_accuracy: 0.9480\n",
      "Epoch 99/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.2552 - accuracy: 0.9215 - val_loss: 0.1959 - val_accuracy: 0.9401\n",
      "Epoch 100/100\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.2904 - accuracy: 0.9100 - val_loss: 0.2315 - val_accuracy: 0.9264\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.7386 - accuracy: 0.7617\n",
      "Accuracy on the test set: 76.17%.\n"
     ]
    }
   ],
   "source": [
    "optimizer = tfa.optimizers.AdamW(\n",
    "    learning_rate=scheduled_lrs,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "vit_model = create_vit_classifier()\n",
    "vit_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "vit_model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)\n",
    "\n",
    "loss, accuracy = vit_model.evaluate(test_ds)\n",
    "accuracy = round(accuracy * 100, 2)\n",
    "print(f\"Accuracy on the test set: {accuracy}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-20 05:14:46.191922: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 100). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: classification_vit_model@acc_76.17/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: classification_vit_model@acc_76.17/assets\n",
      "/opt/conda/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "/opt/conda/lib/python3.7/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n"
     ]
    }
   ],
   "source": [
    "vit_model.save(f\"classification_vit_model@acc_{accuracy}\", include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* https://keras.io/examples/vision/image_classification_with_vision_transformer/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "regular-classification.ipynb",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m84",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m84"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
