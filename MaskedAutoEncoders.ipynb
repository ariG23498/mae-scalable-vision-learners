{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gq8N_kV5J-je"
   },
   "source": [
    "# Masked Autoencoders Are Scalable Vision Learners\n",
    "\n",
    "This notebook is a TF2 implementation of [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377) by He et. al. \n",
    "\n",
    "The notebook uses the following resources as references:\n",
    "\n",
    "* https://keras.io/examples/vision/image_classification_with_vision_transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7R1xBElz3PDk"
   },
   "source": [
    "# Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PFy49HtxKz3r",
    "outputId": "ca994a36-c3dc-425d-a99d-7155c0cb559a"
   },
   "outputs": [],
   "source": [
    "! pip install -q tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AUn7CggVEPer"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Setting seeds for reproducibility.\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vtkCMZIMCcV"
   },
   "outputs": [],
   "source": [
    "# DATA\n",
    "BUFFER_SIZE = 1024\n",
    "BATCH_SIZE = 512\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = (32, 32, 3)\n",
    "NUM_CLASSES = 100\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 10\n",
    "\n",
    "# AUGMENTATION\n",
    "IMAGE_SIZE = 72  # We'll resize input images to this size\n",
    "PATCH_SIZE = 6  # Size of the patches to be extract from the input images\n",
    "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
    "\n",
    "# ENCODER and DECODER\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "PROJECTION_DIM = 64\n",
    "ENC_NUM_HEADS = 4\n",
    "ENC_LAYERS = 8\n",
    "DEC_NUM_HEADS = 4\n",
    "DEC_LAYERS = 8\n",
    "TRANSFORMER_UNITS = [\n",
    "    PROJECTION_DIM * 2,\n",
    "    PROJECTION_DIM,\n",
    "]  # Size of the transformer layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rArMzRYX3REs"
   },
   "source": [
    "# Dataset\n",
    "\n",
    "Using CIFAR100 for our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmsKcx_JEkUt",
    "outputId": "e2d1fe4f-3cfd-4eb5-a462-ffe258e91efb"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "(x_train, y_train), (x_val, y_val) = (\n",
    "    (x_train[:40000], y_train[:40000]),\n",
    "    (x_train[40000:], y_train[40000:]),\n",
    ")\n",
    "print(f\"Training samples: {len(x_train)}\")\n",
    "print(f\"Validation samples: {len(x_val)}\")\n",
    "print(f\"Testing samples: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-rGBAtELs3F"
   },
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train))\n",
    "train_ds = train_ds.batch(BATCH_SIZE).prefetch(AUTO)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_val))\n",
    "val_ds = val_ds.batch(BATCH_SIZE).prefetch(AUTO)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test))\n",
    "test_ds = test_ds.prefetch(AUTO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3AYntKxt3WP5"
   },
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sXDYQCvJF1C5"
   },
   "outputs": [],
   "source": [
    "def get_augmentation_model():\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            layers.Rescaling(1 / 255.0),\n",
    "            layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
    "            layers.RandomFlip(\"horizontal\"),\n",
    "            layers.RandomRotation(factor=0.02),\n",
    "            layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "        ],\n",
    "        name=\"data_augmentation\",\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_test_augmentation_model():\n",
    "    model = keras.Sequential(\n",
    "        [layers.Rescaling(1 / 255.0), layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),],\n",
    "        name=\"test_data_augmentation\",\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kx9cp2o53Z9w"
   },
   "source": [
    "# Create Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "okJv4FwlGOG3"
   },
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "        # Assuming the image has three channels each patch would be\n",
    "        # of size (patch_size, patch_size, 3).\n",
    "        self.resize = layers.Reshape((-1, patch_size * patch_size * 3))\n",
    "\n",
    "    def call(self, images):\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patches = self.resize(patches)\n",
    "        return patches\n",
    "\n",
    "    def show_patched_image(self, images, patches):\n",
    "        # Here we will accept a batch of patches and show a randomly selected image.\n",
    "        idx = np.random.choice(len(patches))\n",
    "        print(f\"Index selected: {idx}.\")\n",
    "        \n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.imshow(keras.utils.array_to_img(images[idx]))\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "        n = int(np.sqrt(NUM_PATCHES))\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        for i, patch in enumerate(patches[idx]):\n",
    "            ax = plt.subplot(n, n, i + 1)\n",
    "            patch_img = tf.reshape(patch, (PATCH_SIZE, PATCH_SIZE, 3))\n",
    "            plt.imshow(keras.utils.img_to_array(patch_img))\n",
    "            plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    # Useful for the training monitor.\n",
    "    # taken from https://stackoverflow.com/a/58082878/10319735\n",
    "    def reconstruct_from_patch(self, patches):\n",
    "        # This reconstructs one image at a time\n",
    "        # does not work with batched images.\n",
    "        patches_tmp = tf.reshape(patches, (NUM_PATCHES, PATCH_SIZE, PATCH_SIZE, 3))\n",
    "        rows = tf.split(patches_tmp, IMAGE_SIZE // PATCH_SIZE, axis=0)\n",
    "        rows = [tf.concat(tf.unstack(x), axis=1) for x in rows]\n",
    "        reconstructed = tf.concat(rows, axis=0)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXIVskF0O1u3"
   },
   "outputs": [],
   "source": [
    "image_batch = next(iter(train_ds))\n",
    "augmentation_model = get_augmentation_model()\n",
    "augmeneted_images = augmentation_model(image_batch)\n",
    "patch_layer = Patches(patch_size=PATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "id": "FZgVpKzZSZso",
    "outputId": "01fe0ad6-97a3-493f-9241-ed27582a7f25"
   },
   "outputs": [],
   "source": [
    "patches = patch_layer(images=augmeneted_images)\n",
    "patch_layer.show_patched_image(augmeneted_images, patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "-kR4A7bMSU7q",
    "outputId": "1a1af25c-39b0-4745-badf-2dd9871c21e1"
   },
   "outputs": [],
   "source": [
    "image = patch_layer.reconstruct_from_patch(patches[348])\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ss7CMuiWGNmw"
   },
   "source": [
    "# Patch Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qC_K3lYKK15g"
   },
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, patch_size, projection_dim, batch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "        self.projection_dim = projection_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # This is a trainable mask token as suggested in the paper.\n",
    "        self.mask_token = tf.Variable(\n",
    "            tf.random.normal([1, patch_size * patch_size * 3])\n",
    "        )\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        (_, self.num_patches, self.patch_area) = input_shape\n",
    "\n",
    "        # Create the projection layer.\n",
    "        self.projection = layers.Dense(units=self.projection_dim)\n",
    "\n",
    "        # Create the positional embedding layer.\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=self.num_patches, output_dim=self.projection_dim\n",
    "        )\n",
    "\n",
    "        # 75% of number of patches should be masked.\n",
    "        self.num_mask = int(0.75 * self.num_patches)\n",
    "\n",
    "        # Create random indices from a uniform distribution and then split\n",
    "        # it into mask and unmask indices.\n",
    "        rand_indices = tf.argsort(\n",
    "            tf.random.uniform(shape=(self.batch_size, self.num_patches)), axis=-1\n",
    "        )\n",
    "        self.mask_indices = rand_indices[:, : self.num_mask]\n",
    "        self.unmask_indices = rand_indices[:, self.num_mask :]\n",
    "\n",
    "    def call(self, patch):\n",
    "        # patch shape = (B, num_patches, p*p*3)\n",
    "        # Get the positional embeddings.\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        pos_embeddings = self.position_embedding(positions[tf.newaxis, ...])\n",
    "        pos_embeddings = tf.tile(\n",
    "            pos_embeddings, [self.batch_size, 1, 1]\n",
    "        )  # (B, num_patches, projection_dim)\n",
    "\n",
    "        # Embed the patches.\n",
    "        patch_embeddings = (\n",
    "            self.projection(patch) + pos_embeddings\n",
    "        )  # (B, num_patches, projection_dim)\n",
    "\n",
    "        # The encoder input is the unmasked patch embeddings.\n",
    "        unmasked_embeddings = tf.gather(\n",
    "            patch_embeddings, self.unmask_indices, axis=1, batch_dims=1\n",
    "        )  # (B, unmask_numbers, projection_dim)\n",
    "\n",
    "        # Get the unmasked and masked positions.\n",
    "        unmasked_positions = tf.gather(\n",
    "            pos_embeddings, self.unmask_indices, axis=1, batch_dims=1\n",
    "        )  # (B, unmask_numbers, projection_dim)\n",
    "        masked_positions = tf.gather(\n",
    "            pos_embeddings, self.mask_indices, axis=1, batch_dims=1\n",
    "        )  # (B, mask_numbers, projection_dim)\n",
    "\n",
    "        # Repeat the mask token number of mask times\n",
    "        # mask tokens replace the masks of the image.\n",
    "        mask_tokens = tf.repeat(self.mask_token, repeats=self.num_mask, axis=0)\n",
    "        mask_tokens = tf.repeat(\n",
    "            mask_tokens[tf.newaxis, ...], repeats=self.batch_size, axis=0\n",
    "        )\n",
    "\n",
    "        # Get the masked embeddings.\n",
    "        masked_embeddings = self.projection(mask_tokens) + masked_positions\n",
    "        return unmasked_embeddings, masked_embeddings, unmasked_positions\n",
    "\n",
    "    def show_masked_image(self, patches):\n",
    "        unmasked_patches = tf.gather(patches, self.unmask_indices, axis=1, batch_dims=1)\n",
    "\n",
    "        # Necessary for plotting.\n",
    "        ids = tf.argsort(self.unmask_indices)\n",
    "        sorted_unmask_indices = tf.sort(self.unmask_indices)\n",
    "        unmasked_patches = tf.gather(unmasked_patches, ids, batch_dims=1)\n",
    "\n",
    "        n = int(np.sqrt(NUM_PATCHES))\n",
    "        unmask_index = sorted_unmask_indices[0]\n",
    "        unmasked_patch = unmasked_patches[0]\n",
    "\n",
    "        plt.figure(figsize=(4, 4))\n",
    "\n",
    "        count = 0\n",
    "        for i in range(NUM_PATCHES):\n",
    "            ax = plt.subplot(n, n, i + 1)\n",
    "\n",
    "            if count < PATCH_SIZE * PATCH_SIZE and unmask_index[count].numpy() == i:\n",
    "                patch = unmasked_patch[count]\n",
    "                patch_img = tf.reshape(patch, (PATCH_SIZE, PATCH_SIZE, 3))\n",
    "                plt.imshow(keras.utils.img_to_array(patch_img))\n",
    "                plt.axis(\"off\")\n",
    "                count = count + 1\n",
    "            else:\n",
    "                patch_img = tf.zeros((PATCH_SIZE, PATCH_SIZE, 3))\n",
    "                plt.imshow(keras.utils.img_to_array(patch_img))\n",
    "                plt.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 710
    },
    "id": "uPsJJ1fcRD8g",
    "outputId": "82980361-9961-48e2-f346-54816cba03db"
   },
   "outputs": [],
   "source": [
    "patch_encoder = PatchEncoder(\n",
    "    patch_size=PATCH_SIZE, projection_dim=PROJECTION_DIM, batch_size=BATCH_SIZE\n",
    ")\n",
    "unmasked_embeddings, masked_embeddings, unmasked_positions = patch_encoder(\n",
    "    patch=patches\n",
    ")\n",
    "patch_encoder.show_masked_image(patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_batch[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sfvIKrUUZE8"
   },
   "source": [
    "# Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lrqZHsrDfnDk"
   },
   "outputs": [],
   "source": [
    "def get_mlp(hidden_units, dropout_rate):\n",
    "    layers_list = []\n",
    "    for units in hidden_units:\n",
    "        layers_list.append(layers.Dense(units, activation=tf.nn.gelu))\n",
    "        layers_list.append(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    model = keras.Sequential(layers_list)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wbVJxZOUcwM"
   },
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n5BRsEC2QaOp"
   },
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, num_layers, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        (_, self.num_unmask, self.projection_dim) = input_shape\n",
    "        self.layer_norm1 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)\n",
    "        self.layer_norm2 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)\n",
    "        self.layer_norm3 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)\n",
    "\n",
    "        self.add1 = layers.Add()\n",
    "        self.add2 = layers.Add()\n",
    "\n",
    "        self.mlp = get_mlp(hidden_units=TRANSFORMER_UNITS, dropout_rate=0.1)\n",
    "\n",
    "        self.mha1 = layers.MultiHeadAttention(\n",
    "            num_heads=self.num_heads,\n",
    "            key_dim=self.projection_dim // self.num_heads,\n",
    "            dropout=0.1,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        for _ in range(self.num_layers):\n",
    "            # Layer normalization 1.\n",
    "            x1 = self.layer_norm1(inputs)\n",
    "            # Create a multi-head attention layer.\n",
    "            attention_output = self.mha1(query=x1, value=x1)\n",
    "            # Skip connection 1.\n",
    "            x2 = self.add1([attention_output, inputs])\n",
    "            # Layer normalization 2.\n",
    "            x3 = self.layer_norm2(x2)\n",
    "            # MLP.\n",
    "            x3 = self.mlp(x3)\n",
    "            # Skip connection 2.\n",
    "            inputs = self.add2([x3, x2])\n",
    "\n",
    "        # Create the encoder ouputs\n",
    "        encoder_outputs = self.layer_norm3(inputs)\n",
    "\n",
    "        return encoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEXmrt1OUeON"
   },
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1b0pdemuTn4K"
   },
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, num_layers, num_heads, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        (_, self.num_patches, self.projection_dim) = input_shape\n",
    "        self.layer_norm1 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)\n",
    "        self.layer_norm2 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)\n",
    "        self.layer_norm3 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)\n",
    "\n",
    "        self.add1 = layers.Add()\n",
    "        self.add2 = layers.Add()\n",
    "\n",
    "        self.mlp = get_mlp(hidden_units=TRANSFORMER_UNITS, dropout_rate=0.1)\n",
    "\n",
    "        self.mha1 = layers.MultiHeadAttention(\n",
    "            num_heads=self.num_heads,\n",
    "            key_dim=self.projection_dim // self.num_heads,\n",
    "            dropout=0.1,\n",
    "        )\n",
    "\n",
    "        self.dense = layers.Dense(\n",
    "            units=self.patch_size * self.patch_size * 3, activation=\"sigmoid\"\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        for _ in range(self.num_layers):\n",
    "            # Layer normalization 1.\n",
    "            x1 = self.layer_norm1(inputs)\n",
    "            # Create a multi-head attention layer.\n",
    "            attention_output = self.mha1(query=x1, value=x1)\n",
    "            # Skip connection 1.\n",
    "            x2 = self.add1([attention_output, inputs])\n",
    "            # Layer normalization 2.\n",
    "            x3 = self.layer_norm2(x2)\n",
    "            # MLP.\n",
    "            x3 = self.mlp(x3)\n",
    "            # Skip connection 2.\n",
    "            inputs = self.add1([x3, x2])\n",
    "\n",
    "        # Create the encoder ouputs.\n",
    "        decoder_outputs = self.layer_norm3(inputs)\n",
    "        decoder_outputs = self.dense(decoder_outputs)\n",
    "        return decoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjwjMjJUUn_i"
   },
   "source": [
    "# MaskedAutoEncoder Model\n",
    "\n",
    "This is the trainer model where we encapsulate the training logic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2mm37Ew-S9X"
   },
   "outputs": [],
   "source": [
    "class MaskedAutoencoder(keras.Model):\n",
    "    def __init__(\n",
    "        self, augmentation_model, patch_layer, patch_encoder, encoder, decoder, **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.augmentation_model = augmentation_model\n",
    "        self.patch_layer = patch_layer\n",
    "        self.patch_encoder = patch_encoder\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def calculate_loss(self, images):\n",
    "        # Augment the input images.\n",
    "        augmeneted_images = self.augmentation_model(images)\n",
    "\n",
    "        # Patch the augmented images.\n",
    "        patches = self.patch_layer(augmeneted_images)\n",
    "\n",
    "        # Encode the patches.\n",
    "        (\n",
    "            unmasked_embeddings,\n",
    "            masked_embeddings,\n",
    "            unmasked_positions,\n",
    "        ) = self.patch_encoder(patches)\n",
    "        encoder_outputs = self.encoder(unmasked_embeddings)\n",
    "\n",
    "        # Create the decoder inputs.\n",
    "        encoder_outputs = encoder_outputs + unmasked_positions\n",
    "        decoder_inputs = tf.concat([encoder_outputs, masked_embeddings], axis=1)\n",
    "\n",
    "        # Decode the inputs.\n",
    "        decoder_outputs = self.decoder(decoder_inputs)\n",
    "\n",
    "        loss_patch = tf.gather(\n",
    "            patches, self.patch_encoder.mask_indices, axis=1, batch_dims=1\n",
    "        )\n",
    "        loss_output = tf.gather(\n",
    "            decoder_outputs, self.patch_encoder.mask_indices, axis=1, batch_dims=1\n",
    "        )\n",
    "\n",
    "        # Compute the total loss.\n",
    "        total_loss = self.compiled_loss(loss_patch, loss_output)\n",
    "\n",
    "        return total_loss, loss_patch, loss_output\n",
    "\n",
    "    def train_step(self, images):\n",
    "        with tf.GradientTape() as tape:\n",
    "            total_loss, loss_patch, loss_output = self.calculate_loss(images)\n",
    "\n",
    "        # Apply gradients.\n",
    "        train_vars = [\n",
    "            self.augmentation_model.trainable_variables,\n",
    "            self.patch_layer.trainable_variables,\n",
    "            self.patch_encoder.trainable_variables,\n",
    "            self.encoder.trainable_variables,\n",
    "            self.decoder.trainable_variables,\n",
    "        ]\n",
    "        grads = tape.gradient(total_loss, train_vars)\n",
    "        tv_list = []\n",
    "        for (grad, var) in zip(grads, train_vars):\n",
    "            for g, v in zip(grad, var):\n",
    "                tv_list.append((g, v))\n",
    "        self.optimizer.apply_gradients(tv_list)\n",
    "\n",
    "        # Report progress.\n",
    "        self.compiled_metrics.update_state(loss_patch, loss_output)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, images):\n",
    "        total_loss, loss_patch, loss_output = self.calculate_loss(images)\n",
    "\n",
    "        # Update the trackers.\n",
    "        self.compiled_metrics.update_state(loss_patch, loss_output)\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model compilation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1S5OCq-QLIoC"
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "augmentation_model = get_augmentation_model()\n",
    "patch_layer = Patches(patch_size=PATCH_SIZE)\n",
    "patch_encoder = PatchEncoder(\n",
    "    patch_size=PATCH_SIZE, projection_dim=PROJECTION_DIM, batch_size=BATCH_SIZE\n",
    ")\n",
    "encoder = Encoder(num_layers=ENC_LAYERS, num_heads=ENC_NUM_HEADS)\n",
    "decoder = Decoder(num_layers=DEC_LAYERS, num_heads=DEC_NUM_HEADS, patch_size=PATCH_SIZE)\n",
    "\n",
    "\n",
    "mae = MaskedAutoencoder(\n",
    "    augmentation_model=augmentation_model,\n",
    "    patch_layer=patch_layer,\n",
    "    patch_encoder=patch_encoder,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    ")\n",
    "\n",
    "optimizer = tfa.optimizers.AdamW(learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "mae.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError(), metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KTFe7p57lADH"
   },
   "outputs": [],
   "source": [
    "# Taking a batch of test inputs to measure model's progress. \n",
    "test_images = next(iter(test_ds))\n",
    "aug_model = get_test_augmentation_model()\n",
    "test_augmeneted_images = aug_model(test_images)\n",
    "\n",
    "\n",
    "class TrainMonitor(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, epoch_interval=None):\n",
    "        self.epoch_interval = epoch_interval\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch_interval and self.epoch_interval % epoch == 0:\n",
    "            test_patches = self.model.patch_layer(test_augmeneted_images)\n",
    "            (\n",
    "                test_unmasked_embeddings,\n",
    "                test_masked_embeddings,\n",
    "                test_unmasked_positions,\n",
    "            ) = self.model.patch_encoder(test_patches)\n",
    "            test_encoder_outputs = self.model.encoder(test_unmasked_embeddings)\n",
    "            test_encoder_outputs = test_encoder_outputs + test_unmasked_positions\n",
    "            test_decoder_inputs = tf.concat(\n",
    "                [test_encoder_outputs, test_masked_embeddings], axis=1\n",
    "            )\n",
    "            test_decoder_outputs = self.model.decoder(test_decoder_inputs)\n",
    "\n",
    "            # Plot the images.\n",
    "            idx = np.random.choice(len(test_patches))\n",
    "            original = self.model.patch_layer.reconstruct_from_patch(test_patches[idx])\n",
    "            reconstructed = self.model.patch_layer.reconstruct_from_patch(\n",
    "                test_decoder_outputs[idx]\n",
    "            )\n",
    "            fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "            ax[0].imshow(tf.keras.preprocessing.image.array_to_img(original))\n",
    "            ax[0].set_title(f\"Original: {epoch:03d}\")\n",
    "\n",
    "            ax[1].imshow(tf.keras.preprocessing.image.array_to_img(reconstructed))\n",
    "            ax[1].set_title(f\"Resonstructed: {epoch:03d}\")\n",
    "\n",
    "            plt.show()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvutXCDrklPo"
   },
   "outputs": [],
   "source": [
    "train_callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=3, verbose=1\n",
    "    ),\n",
    "    TrainMonitor(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-lkHB5r3kQuQ",
    "outputId": "79651f95-1b76-49bb-8027-d73657c0bbf2"
   },
   "outputs": [],
   "source": [
    "history = mae.fit(\n",
    "    train_ds.take(15),\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds.take(5),\n",
    "    callbacks=train_callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9rr_h7O6Q7S_",
    "outputId": "68e10da2-3216-4183-b447-c5f06bfdc492"
   },
   "outputs": [],
   "source": [
    "loss = mae.evaluate(test_ds.take(5))\n",
    "print(f\"Loss: {loss:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO2Arg3X4H2Wu+yzn93y7vR",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "MaskedAutoEncoders.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
